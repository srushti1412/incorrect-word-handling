{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n",
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n",
      "tmp_x shape: (137861, 21, 1)\n",
      "preproc_french_sentences shape: (137861, 21)\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 142s 1s/step - loss: 1.3387 - accuracy: 0.6887 - val_loss: 0.4220 - val_accuracy: 0.8684\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.3292 - accuracy: 0.8963 - val_loss: 0.2075 - val_accuracy: 0.9334\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 144s 1s/step - loss: 0.1927 - accuracy: 0.9393 - val_loss: 0.1412 - val_accuracy: 0.9564\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 144s 1s/step - loss: 0.1361 - accuracy: 0.9573 - val_loss: 0.1045 - val_accuracy: 0.9674\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 145s 1s/step - loss: 0.1089 - accuracy: 0.9659 - val_loss: 0.0900 - val_accuracy: 0.9727\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 150s 1s/step - loss: 0.0904 - accuracy: 0.9718 - val_loss: 0.0820 - val_accuracy: 0.9759\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 145s 1s/step - loss: 0.0771 - accuracy: 0.9760 - val_loss: 0.0735 - val_accuracy: 0.9781\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 146s 1s/step - loss: 0.0696 - accuracy: 0.9783 - val_loss: 0.0673 - val_accuracy: 0.9804\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 147s 1s/step - loss: 0.0633 - accuracy: 0.9804 - val_loss: 0.0673 - val_accuracy: 0.9811\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 146s 1s/step - loss: 0.0585 - accuracy: 0.9819 - val_loss: 0.0639 - val_accuracy: 0.9813\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 21, 256)           51200     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 21, 512)          789504    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 21, 1024)         525312    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 21, 345)          353625    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,719,641\n",
      "Trainable params: 1,719,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Embedding, GRU, Bidirectional, Dropout, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "# Assuming preproc_english_sentences and preproc_french_sentences are already defined\n",
    "# Load Data (this should be adapted to your specific data loading logic)\n",
    "def load_data(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return data.split('\\n')\n",
    "\n",
    "english_sentences = load_data('data/english')\n",
    "french_sentences = load_data('data/french')\n",
    "\n",
    "# Tokenize the sentences\n",
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))\n",
    "\n",
    "preproc_english_sentences, english_tokenizer = tokenize(english_sentences)\n",
    "preproc_french_sentences, french_tokenizer = tokenize(french_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "max_french_sequence_length = max([len(sentence) for sentence in preproc_french_sentences])\n",
    "tmp_x = pad_sequences(preproc_english_sentences, maxlen=max_french_sequence_length, padding='post')\n",
    "preproc_french_sentences = pad_sequences(preproc_french_sentences, maxlen=max_french_sequence_length, padding='post')\n",
    "\n",
    "#pad\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))\n",
    "# Reshape input\n",
    "tmp_x = tmp_x.reshape((-1, max_french_sequence_length, 1))\n",
    "\n",
    "# Check the shapes of the inputs and targets\n",
    "print(f'tmp_x shape: {tmp_x.shape}')\n",
    "print(f'preproc_french_sentences shape: {preproc_french_sentences.shape}')\n",
    "\n",
    "# Vocabulary sizes\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "\n",
    "# Define the model\n",
    "def simple_model(input_shape, english_vocab_size, french_vocab_size):\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "simple_rnn_model = simple_model(tmp_x.shape, english_vocab_size, french_vocab_size)\n",
    "\n",
    "# Train the model\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print the model summary\n",
    "print(simple_rnn_model.summary())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    \n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton:\n",
      "1/1 [==============================] - 1s 662ms/step\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux avril avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediciton:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print('\\nOriginal text:')\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 [==============================] - 77s 686ms/step - loss: 1.7221 - accuracy: 0.5802 - val_loss: 1.1906 - val_accuracy: 0.6557\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 75s 693ms/step - loss: 1.1551 - accuracy: 0.6586 - val_loss: 1.0346 - val_accuracy: 0.6797\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 76s 707ms/step - loss: 1.0363 - accuracy: 0.6775 - val_loss: 0.9362 - val_accuracy: 0.6920\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 79s 732ms/step - loss: 0.9629 - accuracy: 0.6880 - val_loss: 0.8768 - val_accuracy: 0.7073\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 80s 741ms/step - loss: 0.9136 - accuracy: 0.6958 - val_loss: 0.8233 - val_accuracy: 0.7172\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 78s 727ms/step - loss: 0.8766 - accuracy: 0.7024 - val_loss: 0.7876 - val_accuracy: 0.7289\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 75s 699ms/step - loss: 0.8358 - accuracy: 0.7121 - val_loss: 0.7621 - val_accuracy: 0.7348\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 77s 716ms/step - loss: 0.8099 - accuracy: 0.7183 - val_loss: 0.7475 - val_accuracy: 0.7335\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 76s 709ms/step - loss: 0.7888 - accuracy: 0.7232 - val_loss: 0.7156 - val_accuracy: 0.7464\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 81s 750ms/step - loss: 0.7831 - accuracy: 0.7246 - val_loss: 0.6938 - val_accuracy: 0.7526\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 21, 256)          100608    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 21, 1024)         263168    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 21, 345)          353625    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 717,401\n",
      "Trainable params: 717,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the bidirectional model\n",
    "def bd_model(input_shape, english_vocab_size, french_vocab_size):\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "bd_rnn_model = bd_model(tmp_x.shape, english_vocab_size, french_vocab_size)\n",
    "\n",
    "# Train the model\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print the model summary\n",
    "print(bd_rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 21, 256)           51200     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 21, 512)          789504    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 21, 1024)         525312    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 21, 345)          353625    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,719,641\n",
      "Trainable params: 1,719,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 147s 1s/step - loss: 1.2188 - accuracy: 0.7032 - val_loss: 0.4042 - val_accuracy: 0.8673\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 144s 1s/step - loss: 0.3038 - accuracy: 0.9023 - val_loss: 0.1886 - val_accuracy: 0.9401\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.1747 - accuracy: 0.9446 - val_loss: 0.1253 - val_accuracy: 0.9609\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.1250 - accuracy: 0.9609 - val_loss: 0.1008 - val_accuracy: 0.9697\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.0992 - accuracy: 0.9692 - val_loss: 0.0847 - val_accuracy: 0.9743\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.0816 - accuracy: 0.9747 - val_loss: 0.0774 - val_accuracy: 0.9765\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.0728 - accuracy: 0.9773 - val_loss: 0.0738 - val_accuracy: 0.9786\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.0650 - accuracy: 0.9797 - val_loss: 0.0660 - val_accuracy: 0.9810\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.0581 - accuracy: 0.9819 - val_loss: 0.0642 - val_accuracy: 0.9817\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.0579 - accuracy: 0.9820 - val_loss: 0.0601 - val_accuracy: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x240bab606d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the bidirectional model with embedding\n",
    "def bidirectional_embed_model(input_shape, english_vocab_size, french_vocab_size):\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "embed_rnn_model = bidirectional_embed_model(\n",
    "    tmp_x.shape,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "# Print the model summary\n",
    "print(embed_rnn_model.summary())\n",
    "\n",
    "# Train the model\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses, gru_cell_8_layer_call_fn, gru_cell_8_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_to_french_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: english_to_french_model\\assets\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Save the model and related data\n",
    "embed_rnn_model.save('english_to_french_model')\n",
    "\n",
    "# Serialize English Tokenizer to JSON\n",
    "with open('english_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(english_tokenizer.to_json(), ensure_ascii=False))\n",
    "\n",
    "# Serialize French Tokenizer to JSON\n",
    "with open('french_tokenizer.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(french_tokenizer.to_json(), ensure_ascii=False))\n",
    "\n",
    "# Save max lengths\n",
    "max_french_sequence_length_json = max_french_sequence_length\n",
    "with open('sequence_length.json', 'w', encoding='utf8') as f:\n",
    "    f.write(json.dumps(max_french_sequence_length_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEAM SEARCH CODING part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Load the transformer model\n",
    "transformer = tf.keras.models.load_model('transformer_model')\n",
    "\n",
    "# Load the tokenization configuration and vocabulary for English and Spanish\n",
    "with open('eng_vectorization_config.json', 'r', encoding='utf-8') as f:\n",
    "    eng_vectorization_config = json.load(f)\n",
    "with open('eng_vocab.json', 'r', encoding='utf-8') as f:\n",
    "    eng_vocab = json.load(f)\n",
    "\n",
    "with open('spa_vectorization_config.json', 'r', encoding='utf-8') as f:\n",
    "    spa_vectorization_config = json.load(f)\n",
    "with open('spa_vocab.json', 'r', encoding='utf-8') as f:\n",
    "    spa_vocab = json.load(f)\n",
    "\n",
    "# Initialize tokenizers\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.word_index = {word: index for index, word in enumerate(eng_vocab)}\n",
    "\n",
    "spa_tokenizer = Tokenizer()\n",
    "spa_tokenizer.word_index = {word: index for index, word in enumerate(spa_vocab)}\n",
    "\n",
    "# Define the function for vectorizing English input sentences\n",
    "def eng_vectorization(sentences):\n",
    "    sequences = eng_tokenizer.texts_to_sequences(sentences)\n",
    "    return pad_sequences(sequences, maxlen=eng_vectorization_config['max_len'], padding='post')\n",
    "\n",
    "# Define BeamSearchDecoder class\n",
    "class BeamSearchDecoder:\n",
    "    def __init__(self, model, tokenizer, beam_width, max_seq_len):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.beam_width = beam_width\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.start_token = tokenizer.word_index['[start]']\n",
    "        self.end_token = tokenizer.word_index['[end]']\n",
    "        self.vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "    def decode(self, input_seq):\n",
    "        # Initialize the beam with the start token\n",
    "        beam = [(input_seq, [self.start_token], 0)]\n",
    "        \n",
    "        # Loop until the beam reaches the maximum sequence length\n",
    "        for _ in range(self.max_seq_len):\n",
    "            new_beam = []\n",
    "            for input_seq, output_seq, score in beam:\n",
    "                # Expand the current output sequence\n",
    "                output_seq_padded = pad_sequences([output_seq], maxlen=self.max_seq_len, padding='post')\n",
    "                predictions = self.model.predict([input_seq, output_seq_padded], verbose=0)\n",
    "                \n",
    "                # Get the top-k predictions\n",
    "                top_k_indices = np.argsort(predictions[0, len(output_seq)-1, :])[-self.beam_width:]\n",
    "                for index in top_k_indices:\n",
    "                    new_score = score + np.log(predictions[0, len(output_seq)-1, index])\n",
    "                    new_seq = output_seq + [index]\n",
    "                    new_beam.append((input_seq, new_seq, new_score))\n",
    "            \n",
    "            # Keep the best beam_width sequences\n",
    "            beam = sorted(new_beam, key=lambda x: x[2], reverse=True)[:self.beam_width]\n",
    "            \n",
    "            # If the end token is reached, break\n",
    "            if all(seq[-1] == self.end_token for _, seq, _ in beam):\n",
    "                break\n",
    "\n",
    "        # Return the best sequence\n",
    "        return beam[0][1]\n",
    "\n",
    "def sequence_to_text(sequence, tokenizer):\n",
    "    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    return ' '.join([index_to_word.get(index, '') for index in sequence if index != 0 and index != tokenizer.word_index.get('[start]', -1) and index != tokenizer.word_index.get('[end]', -1)])\n",
    "\n",
    "# Translate function using beam search\n",
    "def translate_sentence(sentence, beam_width=3, max_seq_len=20):\n",
    "    input_seq = eng_vectorization([sentence])\n",
    "    beam_search_decoder = BeamSearchDecoder(transformer, spa_tokenizer, beam_width, max_seq_len)\n",
    "    decoded_sequence = beam_search_decoder.decode(input_seq)\n",
    "    return sequence_to_text(decoded_sequence, spa_tokenizer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nullclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
